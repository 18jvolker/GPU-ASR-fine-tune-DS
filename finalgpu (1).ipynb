{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed89bf3-7d67-4ca8-b8f9-3c4e80558ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import string\n",
    "import numpy as np\n",
    "import os \n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import csv\n",
    "from datasets import load_dataset, Dataset\n",
    "from evaluate import load as load_metric \n",
    "from transformers import (\n",
    "    Wav2Vec2Processor,\n",
    "    Wav2Vec2ForCTC,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback,\n",
    "    TrainerCallback \n",
    ")\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "# --- CUSTOM CALLBACK FOR LOGGING RESULTS ---\n",
    "class LoggingCallback(TrainerCallback):\n",
    "    \"\"\"A custom callback to capture and store training and evaluation metrics.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.results = []\n",
    "        self.header = ['epoch', 'step', 'train_loss', 'eval_loss', 'eval_wer', 'runtime']\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        \"\"\"Called when trainer.log() is called (for training metrics).\"\"\"\n",
    "        if logs is not None and state.is_local_process_zero:\n",
    "            if 'loss' in logs:\n",
    "                self.results.append({\n",
    "                    'epoch': round(state.epoch, 2),\n",
    "                    'step': state.global_step,\n",
    "                    'train_loss': logs.get('loss'),\n",
    "                    'eval_loss': None,\n",
    "                    'eval_wer': None,\n",
    "                    'runtime': None,\n",
    "                })\n",
    "\n",
    "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
    "        \"\"\"Called when evaluation results are available.\"\"\"\n",
    "        if metrics is not None and state.is_local_process_zero:\n",
    "            self.results.append({\n",
    "                'epoch': round(state.epoch, 2),\n",
    "                'step': state.global_step,\n",
    "                'train_loss': None,\n",
    "                'eval_loss': metrics.get('eval_loss'),\n",
    "                'eval_wer': metrics.get('eval_wer'),\n",
    "                'runtime': metrics.get('eval_runtime'),\n",
    "            })\n",
    "\n",
    "# --- FFmpeg PATH Setup ---\n",
    "FFMPEG_BIN_PATH = r\"C:\\ffmpeg\\bin\" \n",
    "\n",
    "if os.path.isdir(FFMPEG_BIN_PATH):\n",
    "    os.environ[\"PATH\"] = FFMPEG_BIN_PATH + os.pathsep + os.environ[\"PATH\"]\n",
    "    print(f\"‚úì FFmpeg path injected: {FFMPEG_BIN_PATH}\")\n",
    "else:\n",
    "    print(f\"‚ö† Warning: FFmpeg bin path '{FFMPEG_BIN_PATH}' not found. Relying on System PATH.\")\n",
    "\n",
    "# --- Metric Setup ---\n",
    "try:\n",
    "    wer_metric = load_metric(\"wer\") \n",
    "    print(\"‚úì WER metric loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö† Warning: Could not load WER metric. Error: {e}\")\n",
    "    wer_metric = None\n",
    "\n",
    "# Global processor reference\n",
    "processor = None \n",
    "\n",
    "def compute_metrics(pred):\n",
    "    \"\"\"Calculates the Word Error Rate (WER) for evaluation.\"\"\"\n",
    "    if wer_metric is None or processor is None:\n",
    "        return {\"wer\": 99.0}\n",
    "\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    # Replace padding with processor's pad token ID\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    # Decode predictions and labels\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    # Compute WER\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}\n",
    "\n",
    "\n",
    "# --- Custom Data Collator for CTC ---\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    max_length_labels: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        # Pad audio input features\n",
    "        batch = self.processor.feature_extractor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        \n",
    "        # Pad text labels\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.tokenizer.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                max_length=self.max_length_labels,\n",
    "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "        \n",
    "        # Replace padding with -100 for loss calculation\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "        batch[\"labels\"] = labels\n",
    "        \n",
    "        # Ensure attention mask is present\n",
    "        if \"attention_mask\" not in batch:\n",
    "             batch[\"attention_mask\"] = torch.ones_like(batch[\"input_values\"], dtype=torch.long)\n",
    "             \n",
    "        return batch\n",
    "\n",
    "\n",
    "def load_audio_file(audio_path, target_sr=16000):\n",
    "    \"\"\"Load audio file using soundfile/librosa directly.\"\"\"\n",
    "    try:\n",
    "        # Try soundfile first (faster)\n",
    "        audio_array, sample_rate = sf.read(audio_path)\n",
    "        \n",
    "        # Resample if needed\n",
    "        if sample_rate != target_sr:\n",
    "            # Handle multi-dimensional array (stereo)\n",
    "            if len(audio_array.shape) > 1 and audio_array.shape[1] > 1:\n",
    "                audio_array = audio_array.mean(axis=1)\n",
    "            \n",
    "            audio_array = librosa.resample(audio_array, orig_sr=sample_rate, target_sr=target_sr)\n",
    "        \n",
    "        # Convert to mono if still stereo after potential resampling\n",
    "        if len(audio_array.shape) > 1:\n",
    "            audio_array = audio_array.mean(axis=1)\n",
    "            \n",
    "        # librosa expects float32\n",
    "        if audio_array.dtype != np.float32:\n",
    "            audio_array = audio_array.astype(np.float32)\n",
    "            \n",
    "        return audio_array, target_sr\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö† Error loading {audio_path}: {e}\")\n",
    "        # Return silence as fallback (1 second of silence)\n",
    "        return np.zeros(target_sr, dtype=np.float32), target_sr\n",
    "\n",
    "def write_results_to_csv(results_list, output_dir, header):\n",
    "    \"\"\"Writes the collected metrics to a CSV file.\"\"\"\n",
    "    csv_path = os.path.join(output_dir, \"training_results.csv\")\n",
    "    print(f\"\\nüíæ Saving results to: {csv_path}\")\n",
    "    \n",
    "    try:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        with open(csv_path, 'w', newline='') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=header)\n",
    "            writer.writeheader()\n",
    "            for row in results_list:\n",
    "                filtered_row = {k: v if v is not None else '' for k, v in row.items()}\n",
    "                writer.writerow(filtered_row)\n",
    "        print(f\"‚úì Successfully saved {len(results_list)} metric entries.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to write CSV file: {e}\")\n",
    "\n",
    "\n",
    "def evaluate_and_save_predictions(trainer, eval_dataset, eval_data_raw, output_dir):\n",
    "    \"\"\"Run evaluation and save detailed predictions to CSV.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìä Running Final Evaluation and Generating Predictions CSV\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Get predictions\n",
    "        predictions = trainer.predict(eval_dataset)\n",
    "        pred_logits = predictions.predictions\n",
    "        pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "        \n",
    "        # Decode predictions\n",
    "        pred_str = processor.batch_decode(pred_ids)\n",
    "        \n",
    "        # Get original paths and transcriptions\n",
    "        audio_paths = list(eval_data_raw[\"audio_path\"])\n",
    "        ground_truth = list(eval_data_raw[\"text\"])\n",
    "        \n",
    "        # Ensure lengths match\n",
    "        min_len = min(len(audio_paths), len(ground_truth), len(pred_str))\n",
    "        if min_len < len(audio_paths):\n",
    "            print(f\"‚ö† Warning: Length mismatch detected. Using first {min_len} samples.\")\n",
    "            audio_paths = audio_paths[:min_len]\n",
    "            ground_truth = ground_truth[:min_len]\n",
    "            pred_str = pred_str[:min_len]\n",
    "        \n",
    "        # Clean ground truth the same way as in training\n",
    "        chars_to_remove_regex = string.punctuation\n",
    "        def remove_special_characters(text):\n",
    "            if text is None: return \"\"\n",
    "            text = str(text).lower()\n",
    "            return text.translate(str.maketrans('', '', chars_to_remove_regex))\n",
    "        \n",
    "        ground_truth_cleaned = [remove_special_characters(text) for text in ground_truth]\n",
    "        \n",
    "        # Calculate individual WERs\n",
    "        individual_wers = []\n",
    "        for pred, ref in zip(pred_str, ground_truth_cleaned):\n",
    "            try:\n",
    "                if wer_metric is not None and pred and ref:\n",
    "                    wer = wer_metric.compute(predictions=[pred], references=[ref])\n",
    "                    individual_wers.append(round(wer, 4) if wer is not None else None)\n",
    "                else:\n",
    "                    individual_wers.append(None)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö† Warning: WER calculation failed for one sample: {e}\")\n",
    "                individual_wers.append(None)\n",
    "        \n",
    "        # Create results CSV\n",
    "        results_data = []\n",
    "        for i in range(len(audio_paths)):\n",
    "            results_data.append({\n",
    "                'path': str(audio_paths[i]) if audio_paths[i] is not None else \"\",\n",
    "                'ground_truth': str(ground_truth_cleaned[i]) if ground_truth_cleaned[i] is not None else \"\",\n",
    "                'prediction': str(pred_str[i]) if pred_str[i] is not None else \"\",\n",
    "                'wer': individual_wers[i] if individual_wers[i] is not None else \"\"\n",
    "            })\n",
    "        \n",
    "        # Save to CSV\n",
    "        csv_path = os.path.join(output_dir, \"evaluation_predictions.csv\")\n",
    "        print(f\"üíæ Saving evaluation predictions to: {csv_path}\")\n",
    "        \n",
    "        with open(csv_path, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=['path', 'ground_truth', 'prediction', 'wer'])\n",
    "            writer.writeheader()\n",
    "            writer.writerows(results_data)\n",
    "        print(f\"‚úì Successfully saved {len(results_data)} predictions.\")\n",
    "        \n",
    "        # Print summary statistics\n",
    "        valid_wers = [w for w in individual_wers if w is not None and isinstance(w, (int, float))]\n",
    "        if valid_wers:\n",
    "            avg_wer = sum(valid_wers) / len(valid_wers)\n",
    "            print(f\"\\nüìà Evaluation Summary:\")\n",
    "            print(f\"  Average WER: {avg_wer:.4f}\")\n",
    "            print(f\"  Best WER: {min(valid_wers):.4f}\")\n",
    "            print(f\"  Worst WER: {max(valid_wers):.4f}\")\n",
    "            print(f\"  Samples evaluated: {len(valid_wers)}/{len(results_data)}\")\n",
    "        else:\n",
    "            print(\"‚ö† Warning: No valid WER scores calculated.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during evaluation predictions: {e}\")\n",
    "        print(\"‚ö† Continuing without evaluation predictions CSV...\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "\n",
    "# --- Main Fine-Tuning Function ---\n",
    "def run_wav2vec2_finetune(output_dir: str = \"./wav2vec2-gpu-finetune-model\"):\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üöÄ Starting Wav2Vec2 Fine-tuning Pipeline\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    # Check device\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"üñ•Ô∏è  Device: {device.upper()}\")\n",
    "    if device == \"cpu\":\n",
    "        print(\"‚ö†Ô∏è  Warning: CUDA not found. Training will default to CPU.\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # --- File Paths ---\n",
    "    # NOTE: Update these paths!\n",
    "    DATA_FILE_PATH = r\"C:\\Users\\18jvo\\Desktop\\ASR_Local\\new_audio_paths.csv\"\n",
    "    EVAL_DATA_FILE_PATH = r\"C:\\Users\\18jvo\\eval_pathfinal_file.csv\"\n",
    "    \n",
    "    AUDIO_COLUMN_NAME = \"path\"\n",
    "    TEXT_COLUMN_NAME = \"transcription\"\n",
    "\n",
    "    # Validate file paths\n",
    "    if not os.path.exists(DATA_FILE_PATH):\n",
    "        raise FileNotFoundError(f\"Training data not found: {DATA_FILE_PATH}\")\n",
    "    if not os.path.exists(EVAL_DATA_FILE_PATH):\n",
    "        raise FileNotFoundError(f\"Evaluation data not found: {EVAL_DATA_FILE_PATH}\")\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"‚úì Data files found:\\n  Train: {DATA_FILE_PATH}\\n  Eval:  {EVAL_DATA_FILE_PATH}\\n\")\n",
    "\n",
    "    # Load datasets\n",
    "    print(\"üìä Loading datasets...\")\n",
    "    try:\n",
    "        full_dataset = load_dataset(\"csv\", \n",
    "            data_files={\"train\": DATA_FILE_PATH, \"eval\": EVAL_DATA_FILE_PATH}\n",
    "        )\n",
    "        train_data_raw = full_dataset[\"train\"]\n",
    "        val_data_raw = full_dataset[\"eval\"]\n",
    "        \n",
    "        # NOTE: Using len(train_data_raw) is 324 based on user input\n",
    "        print(f\"‚úì Loaded {len(train_data_raw)} training samples, {len(val_data_raw)} evaluation samples\\n\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to load datasets: {e}\")\n",
    "    \n",
    "    # --- Data Preparation ---\n",
    "    train_data_raw = train_data_raw.rename_column(AUDIO_COLUMN_NAME, \"audio_path\").rename_column(TEXT_COLUMN_NAME, \"text\")\n",
    "    val_data_raw = val_data_raw.rename_column(AUDIO_COLUMN_NAME, \"audio_path\").rename_column(TEXT_COLUMN_NAME, \"text\")\n",
    "\n",
    "    print(\"üìù Creating vocabulary...\")\n",
    "    \n",
    "    chars_to_remove_regex = string.punctuation\n",
    "    \n",
    "    def remove_special_characters(text):\n",
    "        \"\"\"Lowercase and remove punctuation.\"\"\"\n",
    "        if text is None: return \"\"\n",
    "        text = str(text).lower()\n",
    "        return text.translate(str.maketrans('', '', chars_to_remove_regex))\n",
    "        \n",
    "    train_text = list(train_data_raw[\"text\"])\n",
    "    eval_text = list(val_data_raw[\"text\"])\n",
    "    all_text = \" \".join([str(t) for t in train_text + eval_text if t is not None])\n",
    "    cleaned_text = remove_special_characters(all_text)\n",
    "    \n",
    "    # Get unique characters\n",
    "    vocab_list = list(set(cleaned_text.replace(' ', '')))\n",
    "    \n",
    "    # Build vocabulary dictionary\n",
    "    vocab_dict = {v: i for i, v in enumerate([\"|\"] + sorted(vocab_list))}\n",
    "    vocab_dict[\"[UNK]\"] = len(vocab_dict)\n",
    "    vocab_dict[\"[PAD]\"] = len(vocab_dict)\n",
    "    \n",
    "    print(f\"‚úì Vocabulary created with {len(vocab_dict)} tokens\")\n",
    "\n",
    "    # --- Initialize Processor and Model ---\n",
    "    print(\"ü§ñ Initializing model and processor...\")\n",
    "    global processor\n",
    "    processor = Wav2Vec2Processor.from_pretrained(\n",
    "        \"facebook/wav2vec2-base\", \n",
    "        unk_token=\"[UNK]\", \n",
    "        pad_token=\"[PAD]\", \n",
    "        word_delimiter_token=\"|\",\n",
    "        vocab_dict=vocab_dict\n",
    "    )\n",
    "    \n",
    "    model = Wav2Vec2ForCTC.from_pretrained(\n",
    "        \"facebook/wav2vec2-base\",\n",
    "        ctc_loss_reduction=\"mean\",\n",
    "        pad_token_id=processor.tokenizer.pad_token_id,\n",
    "        vocab_size=len(processor.tokenizer)\n",
    "    )\n",
    "    \n",
    "    model.freeze_feature_encoder()\n",
    "    model.to(device) # Move model to device\n",
    "    \n",
    "    print(\"‚úì Model and processor initialized\\n\")\n",
    "    \n",
    "    data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n",
    "\n",
    "    # --- Preprocess Dataset ---\n",
    "    def prepare_dataset(batch):\n",
    "        \"\"\"Preprocess audio and text for training using direct file loading.\"\"\"\n",
    "        audio_path = batch[\"audio_path\"]\n",
    "        audio_array, sample_rate = load_audio_file(audio_path, target_sr=16000)\n",
    "        \n",
    "        # Process audio\n",
    "        batch[\"input_values\"] = processor(audio_array, sampling_rate=sample_rate).input_values[0]\n",
    "        \n",
    "        # Process text\n",
    "        text = batch[\"text\"]\n",
    "        if text is None: text = \"\"\n",
    "        text = str(text).strip()\n",
    "        text = remove_special_characters(text)\n",
    "        \n",
    "        # Manual character-level tokenization\n",
    "        if text == \"\":\n",
    "            label_ids = [processor.tokenizer.encoder[\"|\"]]\n",
    "        else:\n",
    "            text = text.replace(\" \", \"|\")\n",
    "            label_ids = []\n",
    "            for char in text:\n",
    "                if char in processor.tokenizer.encoder:\n",
    "                    label_ids.append(processor.tokenizer.encoder[char])\n",
    "                # Skip characters not in vocab\n",
    "                # Note: The original code handled spaces by replacing them with '|'\n",
    "            \n",
    "        if len(label_ids) == 0:\n",
    "            label_ids = [processor.tokenizer.encoder[\"|\"]]\n",
    "            \n",
    "        batch[\"labels\"] = label_ids\n",
    "        return batch\n",
    "\n",
    "    print(\"‚öôÔ∏è Preprocessing datasets (loading audio files directly)...\")\n",
    "    try:\n",
    "        processed_train_data = train_data_raw.map(\n",
    "            prepare_dataset, \n",
    "            remove_columns=train_data_raw.column_names,\n",
    "            num_proc=1, # Single process for stability\n",
    "            load_from_cache_file=False # CRITICAL: Disable cache\n",
    "        )\n",
    "        processed_val_data = val_data_raw.map(\n",
    "            prepare_dataset, \n",
    "            remove_columns=val_data_raw.column_names,\n",
    "            num_proc=1, # Single process for stability\n",
    "            load_from_cache_file=False # CRITICAL: Disable cache\n",
    "        )\n",
    "        print(\"‚úì Data preprocessed\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Preprocessing failed: {e}\")\n",
    "        raise\n",
    "\n",
    "    # --- Training Configuration (GPU Optimized) ---\n",
    "    print(\"‚öôÔ∏è Configuring training parameters (GPU Optimized)...\")\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        # HYPERPARAMETERS OPTIMIZED FOR GPU\n",
    "        per_device_train_batch_size=16,          # Increased for GPU memory\n",
    "        per_device_eval_batch_size=16,           \n",
    "        gradient_accumulation_steps=2,           # Simulates a large batch size of 32 (16 * 2)\n",
    "        learning_rate=1e-4,                      # Recommended fine-tuning LR\n",
    "        num_train_epochs=20,                     # Safety cap, relying on Early Stopping\n",
    "        \n",
    "        # General Settings\n",
    "        logging_steps=50,                        \n",
    "        save_steps=50, # Save checkpoint more often due to short epochs\n",
    "        evaluation_strategy=\"steps\",             \n",
    "        eval_steps=50, # Evaluate more often\n",
    "        save_strategy=\"steps\",\n",
    "        save_total_limit=2,                      \n",
    "        metric_for_best_model=\"wer\",\n",
    "        load_best_model_at_end=True,             \n",
    "        \n",
    "        # GPU Specific Settings\n",
    "        fp16=True,                               # Enable 16-bit precision for speed\n",
    "        bf16=False,                              \n",
    "        use_cpu=False,                           # Ensure CUDA is used if available\n",
    "        dataloader_num_workers=2,                # Increase if I/O is a bottleneck\n",
    "        report_to=\"none\",\n",
    "        greater_is_better=False,                 \n",
    "    )\n",
    "    \n",
    "    print(\"‚úì GPU Optimized Configuration:\")\n",
    "    print(f\"  Effective Batch Size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "    print(f\"  Learning Rate: {training_args.learning_rate}\")\n",
    "    print(f\"  FP16 Enabled: {training_args.fp16}\")\n",
    "\n",
    "    # Early stopping callback and NEW logging callback\n",
    "    early_stopping_callback = EarlyStoppingCallback(\n",
    "        early_stopping_patience=5,\n",
    "        early_stopping_threshold=0.01 # Stop if WER improvement is less than 1%\n",
    "    )\n",
    "    logging_callback = LoggingCallback()\n",
    "\n",
    "    # --- Training ---\n",
    "    print(\"=\"*60)\n",
    "    print(\"üéØ Starting Training on GPU\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=processed_train_data,\n",
    "        eval_dataset=processed_val_data,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        tokenizer=processor.tokenizer,\n",
    "        feature_extractor=processor.feature_extractor,\n",
    "        callbacks=[early_stopping_callback, logging_callback] \n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚úÖ Training Complete!\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # --- RESULTS SAVING ---\n",
    "    write_results_to_csv(logging_callback.results, output_dir, logging_callback.header)\n",
    "    \n",
    "    # Generate evaluation predictions CSV (wrapped in try-except to not break the pipeline)\n",
    "    try:\n",
    "        evaluate_and_save_predictions(trainer, processed_val_data, val_data_raw, output_dir)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö† Warning: Could not generate evaluation predictions CSV: {e}\")\n",
    "        print(\"Training completed successfully, but predictions CSV was not created.\")\n",
    "    # --- END RESULTS SAVING ---\n",
    "    \n",
    "    print(f\"\\nBest model saved to: {output_dir}\")\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    output_path = os.environ.get(\"WAV2VEC2_OUTPUT_DIR\", \"./wav2vec2-gpu-finetune-model\")\n",
    "    \n",
    "    try:\n",
    "        run_wav2vec2_finetune(output_dir=output_path) \n",
    "        print(f\"\\nüéâ Success! Model and results saved in: {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå FATAL ERROR: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
